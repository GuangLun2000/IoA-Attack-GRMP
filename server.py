# server.py
# This module implements the Server class for federated learning, including model aggregation and defense mechanisms against GRMP attacks.

import torch
import torch.nn as nn
import numpy as np
from typing import List, Dict, Tuple
import copy
from client import BenignClient, AttackerClient
import torch.nn.functional as F


class Server:
    """Server class for federated learning with GRMP attack defense"""
    def __init__(self, global_model: nn.Module, test_loader,
                enable_defense=True, defense_threshold=0.4, total_rounds=20, server_lr=0.8, tolerance_factor=2,
                d_T=0.5, gamma=10.0, similarity_alpha=0.7,
                defense_high_rejection_threshold=0.4, defense_threshold_decay=0.9):
        self.global_model = copy.deepcopy(global_model)
        self.test_loader = test_loader
        self.enable_defense = enable_defense
        self.defense_threshold = defense_threshold
        self.total_rounds = total_rounds
        # CRITICAL: Use explicit cuda:0 instead of 'cuda' to ensure device consistency
        # This prevents issues where 'cuda' and 'cuda:0' are treated as different devices
        if torch.cuda.is_available():
            self.device = torch.device('cuda:0')
        else:
            self.device = torch.device('cpu')
        self.global_model.to(self.device)
        self.clients = []
        self.log_data = []

        # Additional stability parameters
        self.server_lr = server_lr  # Server learning rate (inertia)
        self.tolerance_factor = tolerance_factor  # Defense tolerance level
        
        # Formula 4 constraint parameters (passed to attackers)
        self.d_T = d_T  # Distance threshold for constraint (4b)
        self.gamma = gamma  # Upper bound for constraint (4c)
        self.similarity_alpha = similarity_alpha  # Weight for pairwise similarities
        
        # Adaptive defense parameters
        self.defense_high_rejection_threshold = defense_high_rejection_threshold  # High rejection rate threshold
        self.defense_threshold_decay = defense_threshold_decay  # Threshold decay factor

        # Track historical data for adaptive adjustments
        self.history = {
            'clean_acc': [],  # Clean accuracy
            'rejection_rates': [],  # Client rejection rates
            'local_accuracies': {}  # Local accuracies per client per round {client_id: [acc1, acc2, ...]}
        }

    def register_client(self, client):
        """Register a client to the server."""
        self.clients.append(client)

    def broadcast_model(self):
        """Broadcast the global model to all clients."""
        global_params = self.global_model.get_flat_params()
        # Clone and move to CPU to save GPU memory
        global_params_cpu = global_params.clone().cpu()
        for client in self.clients:
            # set_flat_params works on CPU models
            client.model.set_flat_params(global_params_cpu.clone())
            # Reset optimizer if model is on GPU (rarely needed now)
            if hasattr(client, '_model_on_gpu') and client._model_on_gpu:
                client.reset_optimizer()
            else:
                client.optimizer = None

    def _compute_similarities(self, updates: List[torch.Tensor]) -> np.ndarray:
        """Compute mixed similarities - combining pairwise similarities and similarities with the mean."""
        n_updates = len(updates)

        print("  ðŸ“Š Using mixed similarity computation")

        # Step 1: Compute pairwise similarities
        pairwise_sims = []
        for i in range(n_updates):
            other_sims = []
            for j in range(n_updates):
                if i != j:
                    sim = torch.cosine_similarity(
                        updates[i].unsqueeze(0),
                        updates[j].unsqueeze(0)
                    ).item()
                    other_sims.append(sim)
            # Use average instead of median (more lenient)
            avg_sim = np.mean(other_sims) if other_sims else 0
            pairwise_sims.append(avg_sim)

        # Step 2: Compute similarities with the mean update
        avg_update = torch.stack(updates).mean(dim=0)
        avg_sims = []
        for update in updates:
            sim = torch.cosine_similarity(
                update.unsqueeze(0),
                avg_update.unsqueeze(0)
            ).item()
            avg_sims.append(sim)

        # Step 3: Combine both similarities (weighted average)
        similarities = []
        for i in range(n_updates):
            mixed_sim = self.similarity_alpha * pairwise_sims[i] + (1 - self.similarity_alpha) * avg_sims[i]
            similarities.append(mixed_sim)

        similarities = np.array(similarities)

        # Print information
        print(f"  ðŸ“ˆ Mixed Similarity - Mean: {similarities.mean():.3f}, "
              f"Std Dev: {similarities.std():.3f}")

        # Display similarity for each client
        # Note: similarities are ordered by updates, which match client_ids order from aggregate_updates
        attacker_ids = {client.client_id for client in self.clients if getattr(client, 'is_attacker', False)}
        for i, sim in enumerate(similarities):
            if hasattr(self, '_sorted_client_ids') and i < len(self._sorted_client_ids):
                client_id = self._sorted_client_ids[i]
                client = next((c for c in self.clients if c.client_id == client_id), None)
                if client:
                    client_type = "Attacker" if getattr(client, 'is_attacker', False) else "Benign"
                    print(f"    Client {client_id} ({client_type}): {sim:.3f}")
                else:
                    print(f"    Client {client_id}: {sim:.3f}")
            else:
                print(f"    Update {i}: {sim:.3f}")

        return similarities

    def aggregate_updates(self, updates: List[torch.Tensor],
                          client_ids: List[int]) -> Dict:
        # Store client_ids for similarity display
        self._current_client_ids = client_ids
        self._sorted_client_ids = client_ids
        
        # If defense is disabled, always use standard FedAvg (no defense mechanism)
        if not self.enable_defense:
            # Standard FedAvg aggregation (defense mechanism disabled)
            weights = []
            for cid in client_ids:
                client = self.clients[cid]
                # Use actual data size for weighting (standard FedAvg)
                w = len(getattr(client, 'data_indices', [])) or 1.0
                weights.append(w)
            
            # Weighted aggregation (standard FedAvg)
            dtype = updates[0].dtype
            stacked = torch.stack(updates).to(self.device)
            weight_tensor = torch.tensor(weights, device=self.device, dtype=dtype)
            weight_tensor = weight_tensor / weight_tensor.sum()
            aggregated_update = (stacked * weight_tensor.view(-1, 1)).sum(dim=0)
            aggregated_update_norm = torch.norm(aggregated_update).item()
            del stacked
            
            # Update global model (standard FedAvg: w_t+1 = w_t + Î· * aggregated_update)
            current_params = self.global_model.get_flat_params()
            new_params = current_params + self.server_lr * aggregated_update
            self.global_model.set_flat_params(new_params)
            
            print(f"  ðŸ“Š Standard FedAvg (Defense Disabled): Aggregated {len(updates)}/{len(updates)} updates")
            print(f"  ðŸ”§ Server Learning Rate: {self.server_lr}")
            print(f"  ðŸ“ Aggregated update norm: {aggregated_update_norm:.6f}")
            
            # Return defense log with all clients accepted (for compatibility)
            similarities = torch.ones(len(updates), device=self.device).cpu().numpy()
            defense_log = {
                'similarities': similarities.tolist(),
                'accepted_clients': client_ids.copy(),
                'rejected_clients': [],
                'threshold': 0.0,
                'mean_similarity': 1.0,
                'std_similarity': 0.0,
                'tolerance_factor': self.tolerance_factor,
                'rejection_rate': 0.0,
                'aggregated_update_norm': aggregated_update_norm
            }
            self.history['rejection_rates'].append(0.0)
            return defense_log
        
        # Check if there are any attackers
        has_attackers = any(getattr(self.clients[cid], 'is_attacker', False) for cid in client_ids)
        
        if not has_attackers:
            # Standard FedAvg aggregation (no defense mechanism for baseline experiments)
            # This is the standard federated learning aggregation when there are no attackers
            weights = []
            for cid in client_ids:
                client = self.clients[cid]
                # Use actual data size for weighting (standard FedAvg)
                w = len(getattr(client, 'data_indices', [])) or 1.0
                weights.append(w)
            
            # Weighted aggregation (standard FedAvg)
            dtype = updates[0].dtype
            stacked = torch.stack(updates).to(self.device)
            weight_tensor = torch.tensor(weights, device=self.device, dtype=dtype)
            weight_tensor = weight_tensor / weight_tensor.sum()
            aggregated_update = (stacked * weight_tensor.view(-1, 1)).sum(dim=0)
            aggregated_update_norm = torch.norm(aggregated_update).item()
            del stacked
            
            # Update global model (standard FedAvg: w_t+1 = w_t + Î· * aggregated_update)
            current_params = self.global_model.get_flat_params()
            new_params = current_params + self.server_lr * aggregated_update
            self.global_model.set_flat_params(new_params)
            
            print(f"  ðŸ“Š Standard FedAvg: Aggregated {len(updates)}/{len(updates)} updates")
            print(f"  ðŸ”§ Server Learning Rate: {self.server_lr}")
            print(f"  ðŸ“ Aggregated update norm: {aggregated_update_norm:.6f}")
            
            # Return defense log with all clients accepted (for compatibility)
            similarities = torch.ones(len(updates), device=self.device).cpu().numpy()
            defense_log = {
                'similarities': similarities.tolist(),
                'accepted_clients': client_ids.copy(),
                'rejected_clients': [],
                'threshold': 0.0,
                'mean_similarity': 1.0,
                'std_similarity': 0.0,
                'tolerance_factor': self.tolerance_factor,
                'rejection_rate': 0.0,
                'aggregated_update_norm': aggregated_update_norm
            }
            self.history['rejection_rates'].append(0.0)
            return defense_log
        
        # Defense mechanism (only when there are attackers)
        """
        Aggregate updates - Enhanced stability version.
        Uses a more lenient defense mechanism and smooth update strategy.
        """
        similarities = self._compute_similarities(updates)

        # Compute dynamic threshold (more lenient)
        mean_sim = similarities.mean()
        std_sim = similarities.std()

        # Apply tolerance_factor to make the threshold more lenient
        dynamic_threshold = max(self.defense_threshold,
                                mean_sim - self.tolerance_factor * std_sim)

        # Adaptive adjustment: If rejection rates are too high, further lower the threshold
        if len(self.history['rejection_rates']) > 0:
            recent_rejection_rate = np.mean(self.history['rejection_rates'][-3:])
            if recent_rejection_rate > self.defense_high_rejection_threshold:
                dynamic_threshold *= self.defense_threshold_decay
                print(f"  âš ï¸ High rejection rate detected. Lowering threshold to: {dynamic_threshold:.3f}")

        accepted_indices = []
        rejected_indices = []

        for i, sim in enumerate(similarities):
            if sim >= dynamic_threshold:
                accepted_indices.append(i)
            else:
                rejected_indices.append(i)

        # Record rejection rate
        rejection_rate = len(rejected_indices) / len(updates)
        self.history['rejection_rates'].append(rejection_rate)

        # Aggregate updates from accepted clients
        aggregated_update_norm = 0.0
        if accepted_indices:
            accepted_updates = [updates[i] for i in accepted_indices]
            # Weighted aggregation by claimed data sizes (paper: D_i/D(t)).
            # For benign clients, try to use their data_indices length if available.
            weights = []
            for i in accepted_indices:
                cid = client_ids[i]
                client = self.clients[cid]
                if getattr(client, 'is_attacker', False):
                    w = getattr(client, 'claimed_data_size', 1.0)
                else:
                    w = len(getattr(client, 'data_indices', [])) or 1.0
                weights.append(w)
            # Updates are on CPU, but aggregation can be done on CPU and then moved
            # Move to GPU for computation if needed, or keep on CPU
            dtype = accepted_updates[0].dtype
            # Stack on CPU (updates are on CPU), then move to GPU for weighted sum
            stacked = torch.stack(accepted_updates).to(self.device)  # Move to GPU for aggregation
            weight_tensor = torch.tensor(weights, device=self.device, dtype=dtype)
            weight_tensor = weight_tensor / weight_tensor.sum()
            aggregated_update = (stacked * weight_tensor.view(-1, 1)).sum(dim=0)
            aggregated_update_norm = torch.norm(aggregated_update).item()
            # Clean up stacked tensor immediately
            del stacked

            # Smooth the global model update using server learning rate (key improvement)
            current_params = self.global_model.get_flat_params()
            new_params = current_params + self.server_lr * aggregated_update
            self.global_model.set_flat_params(new_params)

            print(f"  ðŸ“Š Update Stats: Accepted {len(accepted_indices)}/{len(updates)} updates")
            print(f"  ðŸ”§ Server Learning Rate: {self.server_lr} (Smooth updates)")
            print(f"  ðŸ“ Aggregated update norm: {aggregated_update_norm:.6f}")
        else:
            print("  âš ï¸ Warning: No updates were accepted this round!")

        defense_log = {
            'similarities': similarities.tolist(),
            'accepted_clients': [client_ids[i] for i in accepted_indices],
            'rejected_clients': [client_ids[i] for i in rejected_indices],
            'threshold': dynamic_threshold,
            'mean_similarity': mean_sim,
            'std_similarity': std_sim,
            'tolerance_factor': self.tolerance_factor,
            'rejection_rate': rejection_rate,
            'aggregated_update_norm': aggregated_update_norm
        }

        return defense_log

    def evaluate_local_accuracy(self, client) -> float:
        """
        Evaluate local model accuracy for a specific client.
        Uses the global test set for fair comparison across clients.
        
        Memory optimization: Temporarily moves model to GPU for evaluation, then back to CPU.
        """
        # Temporarily move model to GPU for evaluation
        model_was_on_cpu = not getattr(client, '_model_on_gpu', False)
        if model_was_on_cpu:
            client.model.to(self.device)
            client._model_on_gpu = True
        
        try:
            client.model.eval()
            correct = 0
            total = 0
            
            with torch.no_grad():
                # Use global test loader for fair comparison (same test set for all clients)
                for batch in self.test_loader:
                    input_ids = batch['input_ids'].to(self.device)
                    attention_mask = batch['attention_mask'].to(self.device)
                    labels = batch['labels'].to(self.device)
                    
                    outputs = client.model(input_ids, attention_mask)
                    predictions = torch.argmax(outputs, dim=1)
                    
                    correct += (predictions == labels).sum().item()
                    total += labels.size(0)
            
            accuracy = correct / total if total > 0 else 0.0
        finally:
            # Move model back to CPU to free GPU memory
            if model_was_on_cpu:
                client.model.cpu()
                client._model_on_gpu = False
        
        return accuracy
    
    def evaluate(self) -> float:
        """
        Evaluate the global model's performance.
        
        Returns:
            Clean accuracy (float) on the test set
        """
        self.global_model.eval()

        # Evaluate clean accuracy
        correct = 0
        total = 0

        with torch.no_grad():
            for batch in self.test_loader:
                input_ids = batch['input_ids'].to(self.device)
                attention_mask = batch['attention_mask'].to(self.device)
                labels = batch['labels'].to(self.device)

                outputs = self.global_model(input_ids, attention_mask)
                predictions = torch.argmax(outputs, dim=1)

                correct += (predictions == labels).sum().item()
                total += labels.size(0)

        clean_accuracy = correct / total if total > 0 else 0

        # Record historical metrics
        self.history['clean_acc'].append(clean_accuracy)

        return clean_accuracy

    def adaptive_adjustment(self, round_num: int):
        """Adaptively adjust parameters based on historical performance."""
        # Fixed server_lr (no adaptive change)
        pass

    def run_round(self, round_num: int) -> Dict:
        """Execute one round of federated learning - stable version."""
        print(f"\n{'=' * 60}")
        print(f"Round {round_num + 1}/{self.total_rounds}")

        # Adaptive adjustment
        self.adaptive_adjustment(round_num)

        # Display current parameters (no hard-coded stage logic)
        print(f"Current Parameters: server_lr={self.server_lr:.2f}, tolerance={self.tolerance_factor:.1f}")
        print(f"{'=' * 60}")

        # Broadcast the model
        print("ðŸ“¡ Broadcasting the global model...")
        self.broadcast_model()
        
        # Set global model params and constraint parameters for attackers (Formula 4)
        global_params = self.global_model.get_flat_params()  # Already on GPU (server model is on GPU)
        for client in self.clients:
            if isinstance(client, AttackerClient):
                client.set_global_model_params(global_params)
                # Set constraint parameters: d_T and gamma
                # d_T: distance threshold for proximity constraint (4b)
                # gamma: upper bound for aggregation distance constraint (4c)
                client.set_constraint_params(d_T=self.d_T, gamma=self.gamma)

        # Phase 1: Preparation
        print("\nðŸ”§ Phase 1: Client Preparation")
        for client in self.clients:
            client.set_round(round_num)
            if isinstance(client, AttackerClient):
                client.prepare_for_round(round_num)

        # Phase 2: Local Training
        print("\nðŸ’ª Phase 2: Local Training")
        initial_updates = {}
        for client in self.clients:
            update = client.local_train()
            initial_updates[client.client_id] = update
            print(f"  âœ“ Client {client.client_id} completed training")

        # Phase 3: Attacker Camouflage
        print("\nðŸŽ­ Phase 3: Attacker Camouflage")
        benign_updates = []
        for client_id, update in initial_updates.items():
            client = self.clients[client_id]
            if not getattr(client, 'is_attacker', False):
                benign_updates.append(update)
        
        print(f"  Captured {len(benign_updates)} benign updates for camouflage.")
        
        final_updates = {}
        for client_id, update in initial_updates.items():
            client = self.clients[client_id]
            if getattr(client, 'is_attacker', False):
                print(f"  âš ï¸ Triggering camouflage logic for Client {client_id}")
                client.receive_benign_updates(benign_updates)
                final_updates[client_id] = client.camouflage_update(update)
            else:
                final_updates[client_id] = update

        # Phase 4: Defense and Aggregation
        print("\nðŸ›¡ï¸ Phase 4: Defense and Aggregation")
        # Ensure deterministic order of keys
        sorted_client_ids = sorted(final_updates.keys())
        final_update_list = [final_updates[cid] for cid in sorted_client_ids]
        
        defense_log = self.aggregate_updates(final_update_list, sorted_client_ids)

        # Evaluate the global model
        clean_acc = self.evaluate()
        
        # Evaluate local accuracies for each client
        local_accs_this_round = {}
        for client in self.clients:
            try:
                local_acc = self.evaluate_local_accuracy(client)
                local_accs_this_round[client.client_id] = local_acc
                
                # Update history
                if client.client_id not in self.history['local_accuracies']:
                    self.history['local_accuracies'][client.client_id] = []
                self.history['local_accuracies'][client.client_id].append(local_acc)
            except Exception as e:
                # Skip if evaluation fails (e.g., empty data loader)
                print(f"  âš ï¸  Could not evaluate local accuracy for client {client.client_id}: {e}")

        # Defense analysis
        print(f"\nðŸ“ˆ Defense Analysis:")
        print(f"  Dynamic Threshold: {defense_log['threshold']:.4f}")
        print(f"  Rejection Rate: {defense_log['rejection_rate']:.1%}")

        # Create log for the current round
        round_log = {
            'round': round_num + 1,
            'clean_accuracy': clean_acc,
            'acc_diff': (abs(clean_acc - self.history['clean_acc'][-2])
                         if len(self.history['clean_acc']) > 1 else 0.0),
            'defense': defense_log,
            'server_lr': self.server_lr,
            'local_accuracies': local_accs_this_round  # Add local accuracies
        }

        self.log_data.append(round_log)

        # Display results
        print(f"\nðŸ“Š Round {round_num + 1} Results:")
        print(f"  Clean Accuracy: {clean_acc:.4f}")
        # Show performance change
        if len(self.history['clean_acc']) > 1:
            prev_clean = self.history['clean_acc'][-2]
            delta_prev = clean_acc - prev_clean
            best_clean = max(self.history['clean_acc'])
            delta_best = clean_acc - best_clean
            print(f"  Î”Clean vs prev: {delta_prev:+.4f}")
            print(f"  Î”Clean vs best: {delta_best:+.4f}")

        return round_log