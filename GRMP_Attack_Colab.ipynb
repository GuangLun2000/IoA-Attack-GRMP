{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GRMP Attack Experiment - Google Colab\n",
    "\n",
    "This notebook runs the Graph Representation-based Model Poisoning (GRMP) attack experiment on AG News dataset.\n",
    "\n",
    "**Paper**: Graph Representation-based Model Poisoning on the Heterogeneous Internet of Agents\n",
    "\n",
    "## Setup Instructions\n",
    "\n",
    "1. **Enable GPU**: Runtime â†’ Change runtime type â†’ GPU\n",
    "2. **Fetch Code**: Run **Step 0** to clone/download the repo if only this notebook was uploaded.\n",
    "3. **Run all cells**: Runtime â†’ Run all\n",
    "4. **View results**: Check the `results/` folder for outputs and visualizations\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 0: Fetch Code\n",
    "If you only uploaded this notebook, run this to clone the repository and set the working directory.\n",
    "If you've already uploaded the Python files, it will reuse them without cloning.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch repository and set working directory\n",
    "import os, sys, subprocess\n",
    "from pathlib import Path\n",
    "\n",
    "REPO_URL = 'https://github.com/GuangLun2000/IoA-Attack-GRMP.git'\n",
    "REPO_DIR = Path('IoA-Attack-GRMP')\n",
    "\n",
    "def code_files_present():\n",
    "    return Path('main.py').exists() and Path('client.py').exists()\n",
    "\n",
    "if code_files_present():\n",
    "    print('âœ… Code files found in current directory.')\n",
    "else:\n",
    "    if REPO_DIR.exists():\n",
    "        print(f'ðŸ” Using existing folder: {REPO_DIR}')\n",
    "    else:\n",
    "        print(f'ðŸ“¥ Cloning {REPO_URL} ...')\n",
    "        subprocess.run(['git', 'clone', '--depth', '1', REPO_URL], check=True)\n",
    "    os.chdir(REPO_DIR)\n",
    "    print(f\"âœ… Switched to {Path('.').resolve()}\")\n",
    "\n",
    "# Ensure current path is importable for subsequent cells\n",
    "sys.path.append(str(Path('.').resolve()))\n",
    "print(f\"ðŸ“‚ Working directory: {Path('.').resolve()}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Install Dependencies\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "from pathlib import Path\n",
    "req = Path('requirements.txt')\n",
    "if req.exists():\n",
    "    print('Installing from requirements.txt ...')\n",
    "    %pip install -q -r requirements.txt\n",
    "else:\n",
    "    print('requirements.txt not found; installing explicit package list...')\n",
    "    %pip install -q torch>=2.0.0 transformers>=4.35.0 datasets>=2.0.0 numpy>=1.21.0 scikit-learn>=1.0.0 pandas>=1.3.0 tqdm>=4.62.0 matplotlib>=3.4.0 seaborn>=0.11.0\n",
    "\n",
    "print('âœ… Dependencies installed successfully!')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Verify Files and GPU\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if files exist\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "required_files = ['main.py', 'client.py', 'server.py', 'data_loader.py', 'models.py', 'visualization.py']\n",
    "missing_files = [f for f in required_files if not os.path.exists(f)]\n",
    "\n",
    "if missing_files:\n",
    "    print(f\"âš ï¸  Missing files: {missing_files}\")\n",
    "    print(\"Please upload these files to Colab using the file uploader.\")\n",
    "else:\n",
    "    print(\"âœ… All required files found!\")\n",
    "    for f in required_files:\n",
    "        print(f\"  - {f}\")\n",
    "\n",
    "# Check GPU\n",
    "import torch\n",
    "print(f\"\\nPyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU Device: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
    "else:\n",
    "    print(\"âš ï¸  No GPU detected. Training will be slower.\")\n",
    "    print(\"   Go to Runtime â†’ Change runtime type â†’ GPU\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Run Experiment\n",
    "\n",
    "**Note**: All experiment parameters are configured in `main.py`. \n",
    "To modify parameters, edit the `config` dictionary in `main.py` before running this cell.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All experiment parameters are configured in main.py\n",
    "# The experiment will use the config dictionary defined in main.py's main() function\n",
    "print(\"âœ… Using configuration from main.py\")\n",
    "print(\"   To modify parameters, edit the 'config' dictionary in main.py\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the experiment using configuration from main.py\n",
    "import sys\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Import and run main() function which contains all configuration\n",
    "from main import main\n",
    "\n",
    "print(\"ðŸš€ Starting GRMP Attack Experiment...\")\n",
    "print(\"=\" * 60)\n",
    "print(\"Using configuration from main.py\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "try:\n",
    "    # main() function will handle everything: setup, run, and visualization\n",
    "    main()\n",
    "    \n",
    "    print(\"\\nâœ… Experiment completed successfully!\")\n",
    "except Exception as e:\n",
    "    print(f\"\\nâŒ Experiment failed: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: View Results and Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display visualization plots\n",
    "from IPython.display import Image, display\n",
    "from pathlib import Path\n",
    "\n",
    "results_dir = Path(\"results\")\n",
    "# Use default experiment name from main.py (can be changed in main.py config)\n",
    "experiment_name = 'vgae_grmp_attack'\n",
    "\n",
    "# List of all figures to display (4 figures total)\n",
    "figures = [\n",
    "    (\"Figure 1: Global Accuracy and Stability\", f\"{experiment_name}_figure1.png\"),\n",
    "    (\"Figure 2: Cosine Similarity\", f\"{experiment_name}_figure2.png\"),\n",
    "    (\"Figure 3: Local Accuracy (With Attack)\", f\"{experiment_name}_figure3.png\"),\n",
    "    (\"Figure 4: Euclidean Distance\", f\"{experiment_name}_figure4.png\"),\n",
    "]\n",
    "\n",
    "print(\"ðŸ“Š Displaying Visualization Figures:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for fig_title, fig_name in figures:\n",
    "    fig_path = results_dir / fig_name\n",
    "    if fig_path.exists():\n",
    "        print(f\"\\nâœ… {fig_title}\")\n",
    "        display(Image(str(fig_path)))\n",
    "    else:\n",
    "        print(f\"\\nâš ï¸  {fig_title} not found at {fig_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and display experiment results summary with enhanced insights\n",
    "import json\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "\n",
    "results_dir = Path(\"results\")\n",
    "# Use default experiment name from main.py (can be changed in main.py config)\n",
    "experiment_name = 'vgae_grmp_attack'\n",
    "results_path = results_dir / f\"{experiment_name}_results.json\"\n",
    "\n",
    "if results_path.exists():\n",
    "    with open(results_path, 'r') as f:\n",
    "        results_data = json.load(f)\n",
    "    \n",
    "    print(\"ðŸ“Š Experiment Results Summary:\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Display key metrics (using actual metrics from progressive_metrics)\n",
    "    progressive = results_data.get('progressive_metrics', {})\n",
    "    rounds = progressive.get('rounds', [])\n",
    "    clean_acc = progressive.get('clean_acc', [])\n",
    "    acc_diff = progressive.get('acc_diff', [])\n",
    "    rejection_rate = progressive.get('rejection_rate', [])\n",
    "    agg_update_norm = progressive.get('agg_update_norm', [])\n",
    "    log_data = results_data.get('results', [])\n",
    "    \n",
    "    if rounds and clean_acc:\n",
    "        print(f\"\\nðŸŽ¯ Overall Performance Metrics:\")\n",
    "        print(\"-\" * 80)\n",
    "        print(f\"  Total Rounds: {len(rounds)}\")\n",
    "        print(f\"  Initial Clean Accuracy: {clean_acc[0]:.4f} ({clean_acc[0]*100:.2f}%)\")\n",
    "        print(f\"  Final Clean Accuracy: {clean_acc[-1]:.4f} ({clean_acc[-1]*100:.2f}%)\")\n",
    "        print(f\"  Accuracy Change: {clean_acc[-1] - clean_acc[0]:+.4f} ({(clean_acc[-1] - clean_acc[0])*100:+.2f}%)\")\n",
    "        print(f\"  Best Clean Accuracy: {max(clean_acc):.4f} ({max(clean_acc)*100:.2f}%) [Round {rounds[clean_acc.index(max(clean_acc))]}]\")\n",
    "        print(f\"  Worst Clean Accuracy: {min(clean_acc):.4f} ({min(clean_acc)*100:.2f}%) [Round {rounds[clean_acc.index(min(clean_acc))]}]\")\n",
    "        print(f\"  Average Clean Accuracy: {np.mean(clean_acc):.4f} ({np.mean(clean_acc)*100:.2f}%)\")\n",
    "        print(f\"  Std Dev of Clean Accuracy: {np.std(clean_acc):.4f} ({np.std(clean_acc)*100:.2f}%)\")\n",
    "        \n",
    "        if acc_diff:\n",
    "            print(f\"\\n  ðŸ“ˆ Stability Metrics:\")\n",
    "            print(f\"    Final |Î”acc| (Stability): {acc_diff[-1]:.4f}\")\n",
    "            print(f\"    Peak |Î”acc|: {max(acc_diff):.4f} [Round {rounds[acc_diff.index(max(acc_diff))]}]\")\n",
    "            print(f\"    Average |Î”acc|: {np.mean(acc_diff):.4f}\")\n",
    "            print(f\"    Std Dev of |Î”acc|: {np.std(acc_diff):.4f}\")\n",
    "        \n",
    "        if rejection_rate:\n",
    "            total_rejections = sum(rejection_rate)\n",
    "            max_rejection_round = rounds[rejection_rate.index(max(rejection_rate))] if rejection_rate else None\n",
    "            print(f\"\\n  ðŸ›¡ï¸  Defense Mechanism Statistics:\")\n",
    "            print(f\"    Final Rejection Rate: {rejection_rate[-1]:.4f} ({rejection_rate[-1]*100:.2f}%)\")\n",
    "            print(f\"    Average Rejection Rate: {np.mean(rejection_rate):.4f} ({np.mean(rejection_rate)*100:.2f}%)\")\n",
    "            print(f\"    Max Rejection Rate: {max(rejection_rate):.4f} ({max(rejection_rate)*100:.2f}%) [Round {max_rejection_round}]\")\n",
    "            print(f\"    Total Rounds with Rejections: {sum(1 for r in rejection_rate if r > 0)}/{len(rejection_rate)}\")\n",
    "        \n",
    "        if agg_update_norm:\n",
    "            print(f\"\\n  ðŸ“ Update Statistics:\")\n",
    "            print(f\"    Final Aggregated Update Norm: {agg_update_norm[-1]:.6f}\")\n",
    "            print(f\"    Average Update Norm: {np.mean(agg_update_norm):.6f}\")\n",
    "            print(f\"    Max Update Norm: {max(agg_update_norm):.6f} [Round {rounds[agg_update_norm.index(max(agg_update_norm))]}]\")\n",
    "            print(f\"    Min Update Norm: {min(agg_update_norm):.6f} [Round {rounds[agg_update_norm.index(min(agg_update_norm))]}]\")\n",
    "        \n",
    "        # Enhanced defense statistics from log_data\n",
    "        if log_data:\n",
    "            print(f\"\\n  ðŸ” Defense Mechanism Detailed Analysis:\")\n",
    "            all_similarities = []\n",
    "            all_thresholds = []\n",
    "            all_mean_sims = []\n",
    "            all_std_sims = []\n",
    "            total_accepted = 0\n",
    "            total_rejected = 0\n",
    "            \n",
    "            for log in log_data:\n",
    "                defense = log.get('defense', {})\n",
    "                similarities = defense.get('similarities', [])\n",
    "                threshold = defense.get('threshold', 0.0)\n",
    "                mean_sim = defense.get('mean_similarity', 0.0)\n",
    "                std_sim = defense.get('std_similarity', 0.0)\n",
    "                accepted = len(defense.get('accepted_clients', []))\n",
    "                rejected = len(defense.get('rejected_clients', []))\n",
    "                \n",
    "                if similarities:\n",
    "                    all_similarities.extend(similarities)\n",
    "                if threshold is not None:\n",
    "                    all_thresholds.append(threshold)\n",
    "                if mean_sim is not None:\n",
    "                    all_mean_sims.append(mean_sim)\n",
    "                if std_sim is not None:\n",
    "                    all_std_sims.append(std_sim)\n",
    "                total_accepted += accepted\n",
    "                total_rejected += rejected\n",
    "            \n",
    "            if all_similarities:\n",
    "                print(f\"    Similarity Statistics (all clients, all rounds):\")\n",
    "                print(f\"      Mean Similarity: {np.mean(all_similarities):.4f}\")\n",
    "                print(f\"      Std Dev: {np.std(all_similarities):.4f}\")\n",
    "                print(f\"      Min: {np.min(all_similarities):.4f}\")\n",
    "                print(f\"      Max: {np.max(all_similarities):.4f}\")\n",
    "            \n",
    "            if all_thresholds:\n",
    "                print(f\"    Threshold Statistics:\")\n",
    "                print(f\"      Mean Threshold: {np.mean(all_thresholds):.4f}\")\n",
    "                print(f\"      Min Threshold: {np.min(all_thresholds):.4f}\")\n",
    "                print(f\"      Max Threshold: {np.max(all_thresholds):.4f}\")\n",
    "            \n",
    "            if all_mean_sims:\n",
    "                print(f\"    Mean Similarity per Round:\")\n",
    "                print(f\"      Average: {np.mean(all_mean_sims):.4f}\")\n",
    "                print(f\"      Std Dev: {np.std(all_mean_sims):.4f}\")\n",
    "            \n",
    "            print(f\"    Client Acceptance Statistics:\")\n",
    "            print(f\"      Total Accepted: {total_accepted}\")\n",
    "            print(f\"      Total Rejected: {total_rejected}\")\n",
    "            if total_accepted + total_rejected > 0:\n",
    "                acceptance_rate = total_accepted / (total_accepted + total_rejected)\n",
    "                print(f\"      Overall Acceptance Rate: {acceptance_rate:.4f} ({acceptance_rate*100:.2f}%)\")\n",
    "        \n",
    "        # Display ALL per-round summary (no skipping)\n",
    "        print(\"\\nðŸ“ˆ Per-Round Summary (All Rounds):\")\n",
    "        print(\"-\" * 80)\n",
    "        print(f\"{'Round':<7} | {'Clean Acc':<11} | {'|Î”acc|':<9} | {'Rej Rate':<11} | {'Update Norm':<13} | {'Threshold':<11}\")\n",
    "        print(\"-\" * 80)\n",
    "        for i, r in enumerate(rounds):\n",
    "            acc = clean_acc[i] if i < len(clean_acc) else 0.0\n",
    "            diff = acc_diff[i] if i < len(acc_diff) else 0.0\n",
    "            rej = rejection_rate[i] if i < len(rejection_rate) else 0.0\n",
    "            norm = agg_update_norm[i] if i < len(agg_update_norm) else 0.0\n",
    "            threshold = log_data[i].get('defense', {}).get('threshold', 0.0) if i < len(log_data) else 0.0\n",
    "            print(f\"{r:<7} | {acc:<11.4f} | {diff:<9.4f} | {rej:<11.4f} | {norm:<13.6f} | {threshold:<11.4f}\")\n",
    "    else:\n",
    "        print(\"âš ï¸  No progressive metrics found in results\")\n",
    "    \n",
    "    # Enhanced local accuracies display\n",
    "    if 'local_accuracies' in results_data and results_data['local_accuracies']:\n",
    "        local_accs = results_data['local_accuracies']\n",
    "        print(\"\\nðŸ“Š Local Accuracies Analysis:\")\n",
    "        print(\"-\" * 80)\n",
    "        print(f\"{'Client ID':<12} | {'Initial':<11} | {'Final':<11} | {'Change':<11} | {'Average':<11} | {'Std Dev':<11} | {'Best':<11} | {'Worst':<11}\")\n",
    "        print(\"-\" * 80)\n",
    "        \n",
    "        for client_id in sorted(local_accs.keys()):\n",
    "            accs = local_accs[client_id]\n",
    "            if accs:\n",
    "                initial = accs[0]\n",
    "                final = accs[-1]\n",
    "                change = final - initial\n",
    "                avg = np.mean(accs)\n",
    "                std = np.std(accs)\n",
    "                best = max(accs)\n",
    "                worst = min(accs)\n",
    "                print(f\"Client {client_id:<6} | {initial:<11.4f} | {final:<11.4f} | {change:>+11.4f} | {avg:<11.4f} | {std:<11.4f} | {best:<11.4f} | {worst:<11.4f}\")\n",
    "        \n",
    "        # Additional insights\n",
    "        print(f\"\\n  ðŸ“ˆ Cross-Client Statistics:\")\n",
    "        all_final_accs = [accs[-1] for accs in local_accs.values() if accs]\n",
    "        all_initial_accs = [accs[0] for accs in local_accs.values() if accs]\n",
    "        all_avg_accs = [np.mean(accs) for accs in local_accs.values() if accs]\n",
    "        \n",
    "        if all_final_accs:\n",
    "            print(f\"    Final Accuracy - Mean: {np.mean(all_final_accs):.4f}, Std: {np.std(all_final_accs):.4f}\")\n",
    "            print(f\"    Final Accuracy - Best Client: {max(all_final_accs):.4f}, Worst: {min(all_final_accs):.4f}\")\n",
    "            print(f\"    Final Accuracy - Range: {max(all_final_accs) - min(all_final_accs):.4f}\")\n",
    "        \n",
    "        if all_initial_accs:\n",
    "            print(f\"    Initial Accuracy - Mean: {np.mean(all_initial_accs):.4f}, Std: {np.std(all_initial_accs):.4f}\")\n",
    "        \n",
    "        if all_avg_accs:\n",
    "            print(f\"    Average Accuracy (all rounds) - Mean: {np.mean(all_avg_accs):.4f}, Std: {np.std(all_avg_accs):.4f}\")\n",
    "    \n",
    "\n",
    "# Display full configuration summary (all parameters from main.py config)\n",
    "    if 'config' in results_data:\n",
    "        config = results_data['config']\n",
    "        print(\"\\nâš™ï¸  Complete Experiment Configuration:\")\n",
    "        print(\"=\" * 80)\n",
    "        \n",
    "        # Helper function to format config values\n",
    "        def format_value(v):\n",
    "            if v is None:\n",
    "                return 'None'\n",
    "            elif isinstance(v, bool):\n",
    "                return str(v)\n",
    "            elif isinstance(v, float):\n",
    "                return f\"{v:.6f}\".rstrip('0').rstrip('.') if v != int(v) else f\"{int(v)}.0\"\n",
    "            elif isinstance(v, int):\n",
    "                return str(v)\n",
    "            else:\n",
    "                return str(v)\n",
    "        \n",
    "        # 1. Experiment Configuration\n",
    "        print(\"\\nðŸ“‹ Experiment Configuration:\")\n",
    "        print(f\"  experiment_name: {format_value(config.get('experiment_name', 'N/A'))}\")\n",
    "        print(f\"  seed: {format_value(config.get('seed', 'N/A'))}\")\n",
    "        \n",
    "        # 2. Federated Learning Setup\n",
    "        print(\"\\nðŸ‘¥ Federated Learning Setup:\")\n",
    "        print(f\"  num_clients: {format_value(config.get('num_clients', 'N/A'))}\")\n",
    "        print(f\"  num_attackers: {format_value(config.get('num_attackers', 'N/A'))}\")\n",
    "        print(f\"  num_benign_clients: {format_value(config.get('num_benign_clients', 'N/A'))}\")\n",
    "        print(f\"  num_rounds: {format_value(config.get('num_rounds', 'N/A'))}\")\n",
    "        \n",
    "        # 3. Training Mode Configuration\n",
    "        print(\"\\nðŸ”§ Training Mode Configuration:\")\n",
    "        print(f\"  use_lora: {format_value(config.get('use_lora', 'N/A'))}\")\n",
    "        if config.get('use_lora', False):\n",
    "            print(f\"  lora_r: {format_value(config.get('lora_r', 'N/A'))}\")\n",
    "            print(f\"  lora_alpha: {format_value(config.get('lora_alpha', 'N/A'))}\")\n",
    "            print(f\"  lora_dropout: {format_value(config.get('lora_dropout', 'N/A'))}\")\n",
    "            print(f\"  lora_target_modules: {format_value(config.get('lora_target_modules', 'N/A'))}\")\n",
    "        print(f\"  model_name: {format_value(config.get('model_name', 'N/A'))}\")\n",
    "        print(f\"  num_labels: {format_value(config.get('num_labels', 'N/A'))}\")\n",
    "        \n",
    "        # 4. Training Hyperparameters\n",
    "        print(\"\\nðŸ“Š Training Hyperparameters:\")\n",
    "        print(f\"  client_lr: {format_value(config.get('client_lr', 'N/A'))}\")\n",
    "        print(f\"  server_lr: {format_value(config.get('server_lr', 'N/A'))}\")\n",
    "        print(f\"  batch_size: {format_value(config.get('batch_size', 'N/A'))}\")\n",
    "        print(f\"  test_batch_size: {format_value(config.get('test_batch_size', 'N/A'))}\")\n",
    "        print(f\"  local_epochs: {format_value(config.get('local_epochs', 'N/A'))}\")\n",
    "        print(f\"  alpha (FedProx): {format_value(config.get('alpha', 'N/A'))}\")\n",
    "        \n",
    "        # 5. Data Distribution\n",
    "        print(\"\\nðŸ“¦ Data Distribution:\")\n",
    "        print(f\"  dirichlet_alpha: {format_value(config.get('dirichlet_alpha', 'N/A'))}\")\n",
    "        print(f\"  dataset_size_limit: {format_value(config.get('dataset_size_limit', 'N/A'))}\")\n",
    "        \n",
    "        # 6. Attack Configuration\n",
    "        print(\"\\nðŸŽ­ Attack Configuration:\")\n",
    "        print(f\"  attack_start_round: {format_value(config.get('attack_start_round', 'N/A'))}\")\n",
    "        \n",
    "        # 7. Formula 4 Constraint Parameters\n",
    "        print(\"\\nðŸ“ Formula 4 Constraint Parameters:\")\n",
    "        print(f\"  d_T: {format_value(config.get('d_T', 'N/A'))}  # Distance threshold for constraint (4b)\")\n",
    "        print(f\"  gamma: {format_value(config.get('gamma', 'N/A'))}  # Upper bound for constraint (4c)\")\n",
    "        \n",
    "        # 8. Lagrangian Dual Parameters\n",
    "        print(\"\\nðŸ”— Lagrangian Dual Parameters:\")\n",
    "        print(f\"  use_lagrangian_dual: {format_value(config.get('use_lagrangian_dual', 'N/A'))}\")\n",
    "        print(f\"  lambda_init: {format_value(config.get('lambda_init', 'N/A'))}\")\n",
    "        print(f\"  rho_init: {format_value(config.get('rho_init', 'N/A'))}\")\n",
    "        print(f\"  lambda_lr: {format_value(config.get('lambda_lr', 'N/A'))}\")\n",
    "        print(f\"  rho_lr: {format_value(config.get('rho_lr', 'N/A'))}\")\n",
    "        \n",
    "        # 9. VGAE Training Parameters\n",
    "        print(\"\\nðŸ§  VGAE Training Parameters:\")\n",
    "        print(f\"  dim_reduction_size: {format_value(config.get('dim_reduction_size', 'N/A'))}\")\n",
    "        print(f\"  vgae_epochs: {format_value(config.get('vgae_epochs', 'N/A'))}\")\n",
    "        print(f\"  vgae_lr: {format_value(config.get('vgae_lr', 'N/A'))}\")\n",
    "        print(f\"  vgae_hidden_dim: {format_value(config.get('vgae_hidden_dim', 'N/A'))}\")\n",
    "        print(f\"  vgae_latent_dim: {format_value(config.get('vgae_latent_dim', 'N/A'))}\")\n",
    "        print(f\"  vgae_dropout: {format_value(config.get('vgae_dropout', 'N/A'))}\")\n",
    "        \n",
    "        # 10. Attack Optimization Parameters\n",
    "        print(\"\\nâš¡ Attack Optimization Parameters:\")\n",
    "        print(f\"  proxy_step: {format_value(config.get('proxy_step', 'N/A'))}\")\n",
    "        print(f\"  proxy_steps: {format_value(config.get('proxy_steps', 'N/A'))}\")\n",
    "        print(f\"  gsp_perturbation_scale: {format_value(config.get('gsp_perturbation_scale', 'N/A'))}\")\n",
    "        print(f\"  opt_init_perturbation_scale: {format_value(config.get('opt_init_perturbation_scale', 'N/A'))}\")\n",
    "        print(f\"  grad_clip_norm: {format_value(config.get('grad_clip_norm', 'N/A'))}\")\n",
    "        print(f\"  attacker_claimed_data_size: {format_value(config.get('attacker_claimed_data_size', 'N/A'))}\")\n",
    "        \n",
    "        # 11. Proxy Loss Estimation Parameters\n",
    "        print(\"\\nðŸ“‰ Proxy Loss Estimation Parameters:\")\n",
    "        print(f\"  proxy_sample_size: {format_value(config.get('proxy_sample_size', 'N/A'))}\")\n",
    "        print(f\"  proxy_max_batches_opt: {format_value(config.get('proxy_max_batches_opt', 'N/A'))}\")\n",
    "        print(f\"  proxy_max_batches_eval: {format_value(config.get('proxy_max_batches_eval', 'N/A'))}\")\n",
    "        \n",
    "        # 12. Graph Construction Parameters\n",
    "        print(\"\\nðŸ•¸ï¸  Graph Construction Parameters:\")\n",
    "        print(f\"  graph_threshold: {format_value(config.get('graph_threshold', 'N/A'))}\")\n",
    "        \n",
    "        # 13. Defense Mechanism Parameters\n",
    "        print(\"\\nðŸ›¡ï¸  Defense Mechanism Parameters:\")\n",
    "        print(f\"  enable_defense: {format_value(config.get('enable_defense', 'N/A'))}\")\n",
    "        print(f\"  defense_threshold: {format_value(config.get('defense_threshold', 'N/A'))}\")\n",
    "        print(f\"  tolerance_factor: {format_value(config.get('tolerance_factor', 'N/A'))}\")\n",
    "        print(f\"  similarity_alpha: {format_value(config.get('similarity_alpha', 'N/A'))}\")\n",
    "        print(f\"  defense_high_rejection_threshold: {format_value(config.get('defense_high_rejection_threshold', 'N/A'))}\")\n",
    "        print(f\"  defense_threshold_decay: {format_value(config.get('defense_threshold_decay', 'N/A'))}\")\n",
    "        \n",
    "        # 14. Visualization\n",
    "        print(\"\\nðŸ“ˆ Visualization:\")\n",
    "        print(f\"  generate_plots: {format_value(config.get('generate_plots', 'N/A'))}\")\n",
    "        print(f\"  run_both_experiments: {format_value(config.get('run_both_experiments', 'N/A'))}\")\n",
    "        \n",
    "        print(\"\\n\" + \"=\" * 80)\n",
    "else:\n",
    "    print(f\"âš ï¸  Results file not found: {results_path}\")\n",
    "    print(\"   Make sure Step 4 has completed successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Download Results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a zip file with all results\n",
    "import zipfile\n",
    "from pathlib import Path\n",
    "\n",
    "results_dir = Path(\"results\")\n",
    "zip_path = \"grmp_experiment_results.zip\"\n",
    "\n",
    "if results_dir.exists():\n",
    "    with zipfile.ZipFile(zip_path, 'w', zipfile.ZIP_DEFLATED) as zipf:\n",
    "        for file_path in results_dir.rglob('*'):\n",
    "            if file_path.is_file():\n",
    "                zipf.write(file_path, file_path.relative_to(results_dir.parent))\n",
    "    \n",
    "    print(f\"âœ… Created zip file: {zip_path}\")\n",
    "    print(f\"\\nðŸ“¥ Download the file using the cell below\")\n",
    "else:\n",
    "    print(\"âš ï¸  Results directory not found.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download results zip file\n",
    "from google.colab import files\n",
    "\n",
    "if Path(\"grmp_experiment_results.zip\").exists():\n",
    "    files.download('grmp_experiment_results.zip')\n",
    "    print(\"âœ… Download started!\")\n",
    "else:\n",
    "    print(\"âš ï¸  Zip file not found. Run the previous cell first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Exit Running Time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import runtime\n",
    "import time\n",
    "\n",
    "print(\"âœ… All done. All tasks completed.\")\n",
    "\n",
    "runtime.unassign()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
